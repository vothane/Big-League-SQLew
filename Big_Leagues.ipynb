{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Big_Leagues.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPap0gecHb4/PoMPUBcSg+t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vothane/Big-League-SQLew/blob/main/Big_Leagues.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB_Hf1h_jTFu",
        "outputId": "4de6be47-a746-40a2-9547-a0cd5682ba79"
      },
      "source": [
        "# Install java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install spark (change the version number if needed)\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# Unzip the spark file to the current folder\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# Set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "# Install findspark using pip\n",
        "!pip install -q findspark\n",
        "\n",
        "# Spark for Python\n",
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 36 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 63.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=15f6ffe2243dbc2e3d371b4c49b7b587b02e850fcf435d948ac0e6d4f99fff16\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsJ8x2DR3OCU"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2wXPJE_n5L-"
      },
      "source": [
        "class Table:\n",
        "    def __init__(self):\n",
        "        self.rows = []\n",
        "\n",
        "    @property\n",
        "    def get_rows(self):\n",
        "        return self.rows\n",
        "\n",
        "    def insert(self, row_values):\n",
        "        self.rows.append(row_values)\n",
        "\n",
        "class TableBuilder:\n",
        "    @staticmethod\n",
        "    def build_table(url):\n",
        "        html_doc = requests.get(url)\n",
        "        html_content = BeautifulSoup(html_doc.content, 'html.parser')\n",
        "        \n",
        "        raw = html_content.find('thead')\n",
        "        data = raw.find_all('tr')\n",
        "        rows = data[1:]\n",
        "\n",
        "        table = Table()\n",
        "\n",
        "        convert = lambda txt: float(txt) if txt.replace('.', '', 1).isdigit() else txt\n",
        "        \n",
        "        for cols in rows:\n",
        "            col = cols.find_all(\"td\")\n",
        "            table.insert([convert(txt.text) for txt in col])\n",
        "\n",
        "        return table        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ymbCTwImRfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "babafdea-f3ec-4866-845e-7e8d93109672"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# define schema for our data\n",
        "schema = StructType([\n",
        "   StructField(\"Pitch\", StringType(), False),\n",
        "   StructField(\"Count\", FloatType(), False),\n",
        "   StructField(\"Foul/Swing\", FloatType(), False),\n",
        "   StructField(\"Whiff/Swing\", FloatType(), False),\n",
        "   StructField(\"GB/BIP\", FloatType(), False),\n",
        "   StructField(\"LD/BIP\", FloatType(), False),\n",
        "   StructField(\"FB/BIP\", FloatType(), False),\n",
        "   StructField(\"PU/BIP\", FloatType(), False),\n",
        "   StructField(\"GB/FB\", FloatType(), False),\n",
        "   StructField(\"HR/(FB+LD)\", FloatType(), False)])\n",
        "\n",
        "url = \"http://www.brooksbaseball.net/tabs.php?player=456034&var=so\"\n",
        "table = TableBuilder.build_table(url)\n",
        "\n",
        "data = table.get_rows\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Fourseam', 8234.0, 44.58, 24.31, 38.02, 21.76, 30.91, 9.31, 123.01, 6.75], ['Sinker', 11284.0, 43.22, 16.5, 45.69, 21.59, 26.07, 6.65, 175.23, 7.22], ['Change', 5398.0, 30.26, 30.86, 45.23, 24.98, 22.62, 7.17, 200.0, 6.74], ['Slider', 609.0, 41.34, 20.49, 57.01, 16.82, 17.76, 8.41, 321.05, 21.62], ['Curve', 2750.0, 35.0, 26.75, 48.5, 20.32, 24.94, 6.24, 194.44, 10.71], ['Cutter', 4768.0, 41.21, 21.52, 44.51, 24.63, 24.51, 6.34, 181.59, 7.94]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhocl50gr-JN",
        "outputId": "35afbb96-7feb-4d26-e195-1b68e7900d95"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = (SparkSession.builder.appName(\"Big_Leagues\").getOrCreate())\n",
        "\n",
        "sabermetrics = spark.createDataFrame(data, schema)\n",
        " \n",
        "# show David Price most used pitches in descending order\n",
        "(sabermetrics.select(\"Pitch\")\n",
        "             .where(col(\"Count\") > 5000)\n",
        "             .orderBy(desc(\"Count\"))).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|   Pitch|\n",
            "+--------+\n",
            "|  Sinker|\n",
            "|Fourseam|\n",
            "|  Change|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}